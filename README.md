# ğŸ§  autogen-rag-pipeline

> **Agentic RAG: The next-gen, production-ready, fully local Retrieval-Augmented Generation pipelineâ€”built with multi-agent AutoGen orchestration, OpenSearch semantic search, and blazing-fast local LLMs.**

---

## ğŸš€ What is This?

A future-proof RAG stack that *truly thinks for itself*â€”through collaborating generative agents, on your infrastructure, using your data.

**Demo:**
- âœ… Extract unstructured text from structured sources (CSV, Excel, SQL)
- âœ… Dynamically chunk, embed, index and semantically search with OpenSearch
- âœ… Answer queries with *local* open-source LLMs (Mistral via LM Studio, Gemini API) or your favorite API model
- âœ… All orchestrated by AutoGen agents talking in structured chat!

---

## ğŸ¤– How It Works (Agentic Flow)

1. **Ingest:** Upload anythingâ€”Excel/CSV, SQL with hidden text blobs...
2. **Extract:** Multi-agent chat system identifies and extracts â€œmeaningfulâ€ unstructured data scattered anywhere.
3. **Chunk & Embed:** Agents split data into optimal, context-aware chunks. Vectorized locally using `all-MiniLM-L6-v2` or your favorite SentenceTransformer.
4. **Index:** All chunks are indexed in a local OpenSearch vector DB.
5. **Query:** Any question is â€œmulti-hopâ€ handled by agents:
    - Retriever agent finds relevant context chunks (semantic search)
    - LLM agent generates answers using your choice of local LLM/APIs (Gemini/Mistral)
    - Everything is logged, cached, and optionally visualized.

---

## ğŸ”‘ Features at a Glance

- ğŸ¤ **Multi-agent orchestration:** True distributed agent workflows powered by [AutoGen](https://github.com/microsoft/autogen)
- ğŸ—ƒï¸ **Universal data extraction:** Handles embedded text in structured dataâ€”no file format left behind!
- âš¡ **Semantic vector search:** Fast, scalable retrieval via OpenSearch
- ğŸ  **100% local-first:** Full privacy, no vendor lock-in (bring your own LLM via LM Studio or run with Gemini API)
- ğŸ§  **Embeddings:** SOTA SentenceTransformers (default: `all-MiniLM-L6-v2`)
- ğŸŒ **Real-time interface:** (Optional) Streamlit UI for demo/playground
- ğŸ’¸ **Caching, fallback, cost control:** Designed for *production*, not just proof-of-concept.

---

## ğŸ› ï¸ Quickstart (Local Usage)

```
# 1. Clone this repo
git clone https://github.com/dhruv25072003/autogen-rag-pipeline.git
cd autogen-rag-pipeline

# 2. (Recommended) Create a virtualenv
python -m venv venv
# On Windows:
venv\Scripts\activate
# On Mac/Linux:
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Start OpenSearch (local, via Docker)
docker run -d -p 9200:9200 -e "discovery.type=single-node" -e "plugins.security.disabled=true" opensearchproject/opensearch:latest

# 5. Run the pipeline!
python agentic_rag_pipeline.py
# (Replace with your main file)

# 6. (Optional) Launch Streamlit UI ğŸ’»
streamlit run app/streamlit_ui.py
```

---

## ğŸ§ª Example Query

After setup:
- Upload a data file (CSV/Excel, etc.)
- Ask:
    - _â€œSummarize all customer pain points since Aprilâ€_
    - _â€œWhatâ€™s our most frequent support theme by ticket?â€_
    - _â€œWhat did reviewers complain about in Q2?â€_

The pipeline:
- Will extract, chunk, embed, semantic-search, and LLM-answer using your local/private stack

---

## ğŸ§± Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV/Excel  â”‚     â”‚  AutoGen  â”‚         â”‚ SentenceT. â”‚    â”‚ OpenSearch   â”‚
â”‚  SQL Table  â”‚â”€â–ºâ”€â”€â–ºâ”‚  Agents   â”‚â”€Chunkâ”€â–º â”‚ Embeddings â”‚â”€â–ºâ”€â”€â”‚  Vector DB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚                                  â–²
       â”‚<â”€â”€â”€â”€â”€â”€<â”€â”€<â”€â”€User Qâ”€â”€â”€â”€â”˜                                  â”‚
       â–¼                                                          â”‚
   LLM (Local/API) <--- Multi-agent context  <--------------------â”˜
```

All actors communicate by structured group chat. Easily plug in your own LLM & agent logic.

---

## ğŸ“¦ Tech Stack

| Component         | Technology                             |
|-------------------|----------------------------------------|
| RAG Engine        | Python + [AutoGen](https://github.com/microsoft/autogen)      |
| Vector DB         | [OpenSearch (local)](https://opensearch.org/)                 |
| Embeddings        | SentenceTransformers (`all-MiniLM-L6-v2`)          |
| LLM               | Mistral (LM Studio), Gemini API, or any local API |
| UI (optional)     | Streamlit                              |

---

## ğŸŒˆ Why Use This?

- **True "agentic" AI ops:** Showcase, prototype, or deploy cutting-edge agent workflows
- **Full privacy:** All local if you wantâ€”no data leaves your machine!
- **Plug & play:** Swap LLMs, embedding models, or add new agent types without changing data ops code
- **Built for modern MLOps workflows**

---

## â­ Inspiration & Credits

- [AutoGen by Microsoft](https://github.com/microsoft/autogen)
- [LM Studio by lmstudio.ai](https://lmstudio.ai/)
- [SentenceTransformers](https://www.sbert.net/)
- [OpenSearch](https://opensearch.org/)

---

## ğŸ“ License

MIT

---

**Contribute | Star | Try it on your local data â†’ Letâ€™s push agentic retrieval to the next level! ğŸš€**

